# Lightweight Hue configuration file
# ==================================

[desktop]

  # Set this to a random string, the longer the better.
  secret_key=kasdlfjknasdfl3hbaksk3bwkasdfkasdfba23asdf

  # Webserver listens on this address and port
  http_host=0.0.0.0
  http_port=8888

  # Time zone name
  time_zone=Asia/Shanghai

  # Enable or disable debug mode.
  django_debug_mode=false

  # Enable or disable backtrace for server error
  http_500_debug_mode=false

  # app_blacklist=search,hbase,security

  app_blacklist=search,security

  # Webserver runs as this user
  server_user=hue
  server_group=hue

  default_hdfs_superuser=alanding


  # Configuration options for specifying the Desktop Database. For more info,
  # see http://docs.djangoproject.com/en/1.11/ref/settings/#database-engine
  # ------------------------------------------------------------------------
[[database]]
    # Database engine is typically one of:
    # postgresql_psycopg2, mysql, sqlite3 or oracle.
    #
    # Note that for sqlite3, 'name', below is a path to the filename. For other backends, it is the database name
    # Note for Oracle, options={"threaded":true} must be set in order to avoid crashes.
    # Note for Oracle, you can use the Oracle Service Name by setting "host=" and "port=" and then "name=<host>:<port>/<service_name>".
    # Note for MariaDB use the 'mysql' engine.

    # engine=postgresql_psycopg2
    # host=hue-postgres
    # port=5432
    # user=hue
    # password=hue
    # name=hue

    engine=mysql
    host=database
    port=3306
    user=root
    password=root
    name=hue

###########################################################################
# Settings to configure the snippets available in the Notebook
###########################################################################

[notebook]

  # One entry for each type of snippet.
  [[interpreters]]
    # Define the name and how to connect and execute the language.
    # https://docs.gethue.com/administrator/configuration/editor/

    # Example for Docker compose
    [[[mysql]]]
      name = MySQL
      interface=sqlalchemy
      ## https://docs.sqlalchemy.org/en/latest/dialects/mysql.html
      options='{"url": "mysql://root:root@database:3306/hue"}'
      ## options='{"url": "mysql://${USER}:${PASSWORD}@localhost:3306/hue"}'

    [[[hive]]]
      name=Hive
      interface=hiveserver2

    # [[[impala]]]
    #   name=Impala
    #   interface=hiveserver2

    # [[[sql]]]
    #   name=SparkSql
    #   interface=livy

    # [[[spark]]]
    #   name=Scala
    #   interface=livy

    # [[[pyspark]]]
    #   name=PySpark
    #   interface=livy

    # [[[r]]]
    #   name=R
    #   interface=livy

    # [[jar]]]
    #   name=Spark Submit Jar
    #   interface=livy-batch

    # [[[py]]]
    #   name=Spark Submit Python
    #   interface=livy-batch

    # [[[text]]]
    #   name=Text
    #   interface=text

    # [[[markdown]]]
    #   name=Markdown
    #   interface=text

    # [[[sqlite]]]
    #   name = SQLite
    #   interface=rdbms

    # [[[postgresql]]]
    #   name = PostgreSQL
    #   interface=rdbms

    # [[[oracle]]]
    #   name = Oracle
    #   interface=rdbms

    # [[[solr]]]
    #   name = Solr SQL
    #   interface=solr
    #   ## Name of the collection handler
    #   # options='{"collection": "default"}'

    # [[[pig]]]
    #   name=Pig
    #   interface=oozie

    # [[[java]]]
    #   name=Java
    #   interface=oozie

    # [[[spark2]]]
    #   name=Spark
    #   interface=oozie

    # [[[mapreduce]]]
    #   name=MapReduce
    #   interface=oozie

    # [[[sqoop1]]]
    #   name=Sqoop1
    #   interface=oozie

    # [[[distcp]]]
    #   name=Distcp
    #   interface=oozie

    # [[[shell]]]
    #   name=Shell
    #   interface=oozie

    # [[[presto]]]
    #   name=Presto SQL
    #   interface=presto
    #   ## Specific options for connecting to the Presto server.
    #   ## The JDBC driver presto-jdbc.jar need to be in the CLASSPATH environment variable.
    #   ## If 'user' and 'password' are omitted, they will be prompted in the UI.
    #   options='{"url": "jdbc:presto://localhost:8080/catalog/schema", "driver": "io.prestosql.jdbc.PrestoDriver", "user": "root", "password": "root"}'

    # [[[clickhouse]]]
    #   name=ClickHouse
    #   interface=jdbc
    #   ## Specific options for connecting to the ClickHouse server.
    #   ## The JDBC driver clickhouse-jdbc.jar and its related jars need to be in the CLASSPATH environment variable.
    #   options='{"url": "jdbc:clickhouse://localhost:8123", "driver": "ru.yandex.clickhouse.ClickHouseDriver", "user": "readonly", "password": ""}'


[dashboard]

  # Activate the SQL Dashboard (beta).
  has_sql_enabled=true


 [hadoop]

  # Configuration for HDFS NameNode
  # ------------------------------------------------------------------------
  [[hdfs_clusters]]
    # HA support by using HttpFs

    [[[default]]]
      # Enter the filesystem uri
      fs_defaultfs=hdfs://hadoop100:8020

      # NameNode logical name.
      logical_name=mycluster

      # Use WebHdfs/HttpFs as the communication mechanism.
      # Domain should be the NameNode or HttpFs host.
      # Default port is 14000 for HttpFs.
      # 直接用ip，要不会报错
      webhdfs_url=http://192.168.31.76:50070/webhdfs/v1

      # HADOOP的一些配置
      hadoop_conf_dir=/home/alanding/software/bigdata/hadoop/etc/hadoop
      hadoop_hdfs_home=/home/alanding/software/bigdata/hadoop
      hadoop_bin=/home/alanding/software/bigdata/hadoop/bin

  # Configuration for YARN (MR2)
  # ------------------------------------------------------------------------
  [[yarn_clusters]]

    [[[default]]]
      # Enter the host on which you are running the ResourceManager
      resourcemanager_host=192.168.31.76

      # The port where the ResourceManager IPC listens on
      resourcemanager_port=8032

      # Resource Manager logical name (required for HA)
      logical_name=cluster-yarn1

      # Whether to submit jobs to this cluster
      submit_to=True

      # URL of the ResourceManager API
      resourcemanager_api_url=http://192.168.31.76:8088

      # URL of the ProxyServer API
      proxy_api_url=http://192.168.31.76:8088

      # URL of the HistoryServer API
      history_server_api_url=http://192.168.31.149:19888

      # URL of the Spark History Server
      ## spark_history_server_url=http://localhost:18088


###########################################################################
# Settings to configure Beeswax with Hive
###########################################################################

[beeswax]

  # Host where HiveServer2 is running.
  # If Kerberos security is enabled, use fully-qualified domain name (FQDN).
  hive_server_host=192.168.31.76

  # Port where HiveServer2 Thrift server runs on.
  hive_server_port=10000

  # Http thrift port for HiveServer2.
  hive_server_http_port=10001

  hive_conf_dir=/home/alanding/software/bigdata/hive/conf

  # Host where Hive Metastore Server (HMS) is running.
  # If Kerberos security is enabled, the fully-qualified domain name (FQDN) is required.
  hive_metastore_host=192.168.31.76

  # Configure the port the Hive Metastore Server runs on.
  hive_metastore_port=9083

  # Thrift version to use when communicating with HiveServer2.
  # Version 11 comes with Hive 3.0. If issues, try 7.
  thrift_version=7

  # Override the default desktop username and password of the hue user used for authentications with other services.
  # e.g. Used for LDAP/PAM pass-through authentication.
#  auth_username=alanding
#  auth_password=193728456

###########################################################################
# Settings to configure Impala
###########################################################################

[impala]
  # Host of the Impala Server (one of the Impalad)
  ## server_host=localhost

  # Port of the Impala Server
  ## server_port=21050


###########################################################################
# Settings to configure the Spark application.
###########################################################################

[spark]
  # The Livy Server URL.
  ## livy_server_url=http://localhost:8998

  # Configure Livy to start in local 'process' mode, or 'yarn' workers.
  ## livy_server_session_kind=yarn

  # Whether Livy requires client to perform Kerberos authentication.
  ## security_enabled=false

  # Host of the Sql Server
  ## sql_server_host=localhost

  # Port of the Sql Server
  ## sql_server_port=10000

  # Choose whether Hue should validate certificates received from the server.
  ## ssl_cert_ca_verify=true


###########################################################################
# Settings to configure HBase Browser
###########################################################################

[hbase]
  # Comma-separated list of HBase Thrift servers for clusters in the format of '(name|host:port)'.
  hbase_clusters=(Cluster|192.168.31.76:9090)
  hbase_conf_dir=/home/alanding/software/bigdata/hbase/conf


###########################################################################
# Settings to configure Solr Search
###########################################################################

[search]

  # URL of the Solr Server
  ## solr_url=http://192.168.31.76:8983/solr/


###########################################################################
# Settings to configure liboozie
###########################################################################

[liboozie]
  # The URL where the Oozie service runs on. This is required in order for
  # users to submit jobs. Empty value disables the config check.
  oozie_url=http://192.168.31.76:11000/oozie

  remote_deployement_dir=/user/alanding/hue/oozie-apps

###########################################################################
# Settings for the AWS lib
###########################################################################

[aws]
  [[aws_accounts]]
    # Default AWS account
    ## [[[default]]]
      # AWS credentials
      ## access_key_id=
      ## secret_access_key=
      ## security_token=

      # Execute this script to produce the AWS access key ID.
      ## access_key_id_script=/path/access_key_id.sh

      # Execute this script to produce the AWS secret access key.
      ## secret_access_key_script=/path/secret_access_key.sh

      # Allow to use either environment variables or
      # EC2 InstanceProfile to retrieve AWS credentials.
      ## allow_environment_credentials=yes

      # AWS region to use, if no region is specified, will attempt to connect to standard s3.amazonaws.com endpoint
      ## region=us-east-1

      # Endpoint overrides
      ## host=

      # Proxy address and port
      ## proxy_address=
      ## proxy_port=8080
      ## proxy_user=
      ## proxy_pass=

      # Secure connections are the default, but this can be explicitly overridden:
      ## is_secure=true


###########################################################################
# Settings for the Azure lib
###########################################################################
[azure]
  [[azure_accounts]]
    # Default Azure account
    [[[default]]]
      # Azure credentials
      ## client_id=
      # Execute this script to produce the ADLS client id.
      ## client_id_script=/path/client_id.sh
      ## client_secret=
      # Execute this script to produce the ADLS client secret.
      ## client_secret_script=/path/client_secret.sh
      ## tenant_id=
      # Execute this script to produce the ADLS tenant id.
      ## tenant_id_script=/path/tenant_id.sh

  [[adls_clusters]]
    # Default ADLS cluster
    [[[default]]]
      ## fs_defaultfs=adl://<account_name>.azuredatalakestore.net
      ## webhdfs_url=https://<account_name>.azuredatalakestore.net/webhdfs/v1


###########################################################################
# Settings to configure the ZooKeeper Lib
###########################################################################

[libzookeeper]
  # ZooKeeper ensemble. Comma separated list of Host/Port.
  # e.g. localhost:2181,localhost:2182,localhost:2183
  host_ports=192.168.31.76:2181,192.168.31.21:2181,192.168.31.149:2181


###########################################################################
# Settings to configure Kafka
###########################################################################

[kafka]

  [[kafka]]
    # Enable the Kafka integration.
    # is_enabled=false

    # Base URL of Kafka REST API.
    # api_url=http://192.16831.76:8082


###########################################################################
# Settings to configure Metadata
###########################################################################

[metadata]

  [[navigator]]
    # Navigator API URL (without version suffix).
    ## api_url=http://localhost:7187/api

    # Which authentication to use: CM or external via LDAP or SAML.
    ## navmetadataserver_auth_type=CMDB

    # Username of the CM user used for authentication.
    ## navmetadataserver_cmdb_user=hue
    # CM password of the user used for authentication.
    ## navmetadataserver_cmdb_password=
    # Execute this script to produce the CM password. This will be used when the plain password is not set.
    # navmetadataserver_cmdb_password_script=


###########################################################################
# Settings to configure the cozie app
############################c##############################################

[oozie]
  # Location on local FS where the examples are stored.
  local_data_dir=/home/alanding/software/bigdata/oozie/examples

  # Location on local FS where the data for the examples is stored.
  sample_data_dir=/home/alanding/software/bigdata/oozie/oozie-examples-data

  # Location on HDFS where the oozie examples and workflows are stored.
  # Parameters are $TIME and $USER, e.g. /user/$USER/hue/workspaces/workflow-$TIME
  remote_data_dir=/user/alanding/hue/oozie-apps

  # Maximum of Oozie workflows or coodinators to retrieve in one API call.
  ## oozie_jobs_count=100

  # Use Cron format for defining the frequency of a Coordinator instead of the old frequency number/unit.
  enable_cron_scheduling=true

  # Flag to enable the saved Editor queries to be dragged and dropped into a workflow.
  ## enable_document_action=true

  # Flag to enable Oozie backend filtering instead of doing it at the page level in Javascript. Requires Oozie 4.3+.
  ## enable_oozie_backend_filtering=true

  # Flag to enable the Impala action.
  ## enable_impala_action=false

  # Flag to enable the Altus action.
  ## enable_altus_action=false
