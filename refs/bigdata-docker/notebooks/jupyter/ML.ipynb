{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation of ML embeddings\n",
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.bdgenomics.adam:adam-core-spark2_2.11:0.24.0 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree-tmp-dir4005346743236264489/toree_add_deps/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /tmp/toree-tmp-dir4005346743236264489/toree_add_deps/https/repo1.maven.org/maven2/org/bdgenomics/adam/adam-core-spark2_2.11/0.24.0/adam-core-spark2_2.11-0.24.0.jar\n",
      "-> New file at /tmp/toree-tmp-dir4005346743236264489/toree_add_deps/https/repo1.maven.org/maven2/org/bdgenomics/adam/adam-core-spark2_2.11/0.24.0/adam-core-spark2_2.11-0.24.0.pom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking comp.bio.aging:adam-playground_2.11:0.0.13 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree-tmp-dir4005346743236264489/toree_add_deps/\n",
      "-> https://dl.bintray.com/comp-bio-aging/main/\n",
      "-> https://repo1.maven.org/maven2\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.bdgenomics.adam adam-core-spark2_2.11 0.24.0\n",
    "%AddDeps comp.bio.aging adam-playground_2.11 0.0.13 --repository https://dl.bintray.com/comp-bio-aging/main/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.apache.hadoop:hadoop-azure:2.7.6 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree-tmp-dir4005346743236264489/toree_add_deps/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /tmp/toree-tmp-dir4005346743236264489/toree_add_deps/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/2.7.6/hadoop-azure-2.7.6.jar\n",
      "-> New file at /tmp/toree-tmp-dir4005346743236264489/toree_add_deps/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/2.7.6/hadoop-azure-2.7.6.pom\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Waiting for a Spark session to start..."
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking com.microsoft.azure:azure-storage:2.0.0 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree-tmp-dir4005346743236264489/toree_add_deps/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /tmp/toree-tmp-dir4005346743236264489/toree_add_deps/https/repo1.maven.org/maven2/com/microsoft/azure/azure-storage/2.0.0/azure-storage-2.0.0.jar\n",
      "-> New file at /tmp/toree-tmp-dir4005346743236264489/toree_add_deps/https/repo1.maven.org/maven2/com/microsoft/azure/azure-storage/2.0.0/azure-storage-2.0.0.pom\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.apache.hadoop hadoop-azure 2.7.6\n",
    "%AddDeps com.microsoft.azure azure-storage 2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General functions\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  org.apache.spark._\n",
    "import org.apache.spark.sql.{DataFrame, Encoders, SparkSession}\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import scala.reflect.runtime.universe._\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.rdd._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkHadoopConf: (sc: org.apache.spark.SparkContext, acountName: String, accountKey: String)org.apache.spark.SparkContext\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def sparkHadoopConf(sc: SparkContext, acountName: String, accountKey: String) = {\n",
    "  sc.hadoopConfiguration.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "  sc.hadoopConfiguration.set(\"fs.azure.account.key.\" + acountName + \".blob.core.windows.net\", accountKey)\n",
    "  sc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azurize: (container: String, accountName: String, blobFile: String)String\n",
       "writeText2Azure: [T](rdd: org.apache.spark.rdd.RDD[T], container: String, accountName: String, blobFile: String)String\n",
       "writeTsv2Azure: (df: org.apache.spark.sql.DataFrame, container: String, accountName: String, blobFile: String)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def azurize(container: String, accountName: String, blobFile: String): String = \"wasbs://\"+container+\"@\"+accountName+\".blob.core.windows.net/\"+blobFile \n",
    "\n",
    "def writeText2Azure[T]( rdd: RDD[T], container: String, accountName: String, blobFile: String ): String =\n",
    "{\n",
    "  val url = azurize(container, accountName, blobFile)\n",
    "  rdd.saveAsTextFile(url)\n",
    "  url\n",
    "}\n",
    "\n",
    "def writeTsv2Azure( df: DataFrame, container: String, accountName: String, blobFile: String ): String =\n",
    "{\n",
    "  val url = azurize(container, accountName, blobFile)\n",
    "  df.write.option(\"sep\",\"\\t\").option(\"header\",\"true\").csv(url)\n",
    "  url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "account = pipelines1\n",
       "key = \n",
       "connString = DefaultEndpointsProtocol=https;AccountName=pipelines1;AccountKey=;EndpointSuffix=core.windows.net\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "az: (path: String)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "DefaultEndpointsProtocol=https;AccountName=pipelines1;AccountKey=;EndpointSuffix=core.windows.net"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val account = \"pipelines1\"\n",
    "val key = \"\"\n",
    "val connString = s\"DefaultEndpointsProtocol=https;AccountName=pipelines1;AccountKey=${key};EndpointSuffix=core.windows.net\"\n",
    "def az(path: String): String = azurize(\"storage\", account, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkContext = org.apache.spark.SparkContext@66f35057\n",
       "spark = org.apache.spark.sql.SparkSession@1ed54776\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://9555d1d78bab:4041)\" target=\"new_tab\">Spark UI: app-20180618221152-0002</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark app-20180618221152-0002: Some(http://9555d1d78bab:4041)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkContext = sc\n",
    "sparkHadoopConf(sparkContext, account, key)\n",
    "  \n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"mapping_models\")\n",
    "  .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toDouble = UserDefinedFunction(<function1>,DoubleType,Some(List(StringType)))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "UserDefinedFunction(<function1>,DoubleType,Some(List(StringType)))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "import spark.implicits._\n",
    "\n",
    "val toDouble = udf[Double, String]( _.toDouble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write: (df: org.apache.spark.sql.DataFrame, url: String, header: Boolean)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def write(df: DataFrame, url: String, header: Boolean = true) = {\n",
    "  df.coalesce(1).write.option(\"sep\",\"\\t\").option(\"header\", header).csv(url)\n",
    "  url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readTSV: (path: String, header: Boolean, sep: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def readTSV(path: String, header: Boolean = false, sep: String = \"\\t\"): DataFrame = spark.read\n",
    "    .option(\"sep\", sep)\n",
    "    .option(\"comment\", \"#\")\n",
    "    .option(\"inferSchema\", true)\n",
    "    .option(\"header\", header)\n",
    "    .option(\"ignoreLeadingWhiteSpace\", true)\n",
    "    .option(\"ignoreTrailingWhiteSpace\", true)\n",
    "    .option(\"ignoreTrailingWhiteSpace\", true)\n",
    "    .option(\"maxColumns\", 150000)\n",
    "    .csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toVectors: (dataFrame: org.apache.spark.sql.DataFrame, columns: Seq[String], output: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    " def toVectors(dataFrame: DataFrame, columns: Seq[String], output: String) = {  \n",
    "      import org.apache.spark.ml.feature.VectorAssembler\n",
    "      import org.apache.spark.ml.linalg.Vectors\n",
    "      val assembler = new VectorAssembler()\n",
    "        .setInputCols(columns.toArray)\n",
    "        .setOutputCol(output)\n",
    "      assembler.transform(dataFrame.na.fill(0.0, columns).na.fill(0.0)).select(output)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fitPCA: (dataFrame: org.apache.spark.sql.DataFrame, columns: Seq[String], k: Int)(implicit sparkSession: org.apache.spark.sql.SparkSession)org.apache.spark.ml.feature.PCAModel\n",
       "doPCA: (dataFrame: org.apache.spark.sql.DataFrame, columns: Seq[String], k: Int)(implicit sparkSession: org.apache.spark.sql.SparkSession)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature._\n",
    "def fitPCA(dataFrame: DataFrame, columns: Seq[String], k: Int)(implicit sparkSession: SparkSession): PCAModel = {\n",
    "  val df = toVectors(dataFrame, columns, \"features\")\n",
    "  new PCA()\n",
    "    .setInputCol(\"features\")\n",
    "    .setOutputCol(\"PCA\")\n",
    "    .setK(k)\n",
    "    .fit(df)\n",
    "}\n",
    "\n",
    "def doPCA(dataFrame: DataFrame, columns: Seq[String], k: Int)(implicit sparkSession: SparkSession): DataFrame = {\n",
    "  val pca = fitPCA(dataFrame,columns, k)\n",
    "  pca.transform(dataFrame)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTEX-related code\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expressionsPath = expressions\n",
       "byGoPath = expressions/go\n",
       "comparison = expressions/go/gray_whale_with_bowhead_with_minke_with_NMR_with_human_with_mouse_with_cow_full_outer_counts_extended.tsv\n",
       "grouped = expressions/go/grouped\n",
       "ranked = expressions/go/grouped/ranked\n",
       "transcriptsPath = expressions/transcripts\n",
       "validationPath = expressions/validation\n",
       "mouseValidationPath = expressions/validation/mouse\n",
       "jenageValidationPath = expressions/validation/mouse/GSE75192\n",
       "mouseColsValidationPath = expressions/validation/mouse/GSE75192/expressions_columns_GSE75192.tsv\n",
       "mouseRowsValidationPath = expressions/validation/mouse/GSE75192/expressions_rows_GSE75192.tsv\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "expressions/validation/mouse/GSE75192/expressions_rows_GSE75192.tsv"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//for testing\n",
    "val expressionsPath = \"expressions\"\n",
    "val byGoPath = expressionsPath + \"/go\"\n",
    "val comparison = byGoPath + \"/gray_whale_with_bowhead_with_minke_with_NMR_with_human_with_mouse_with_cow_full_outer_counts_extended.tsv\"\n",
    "val grouped = byGoPath + \"/grouped\"\n",
    "val ranked = byGoPath + \"/grouped/ranked\"\n",
    "val transcriptsPath = expressionsPath + \"/transcripts\"\n",
    "\n",
    "val validationPath = expressionsPath + \"/\" + \"validation\"\n",
    "val mouseValidationPath = validationPath + \"/\" + \"mouse\"\n",
    "val jenageValidationPath = mouseValidationPath + \"/\" + \"GSE75192\"\n",
    "val mouseColsValidationPath = jenageValidationPath + \"/\" + \"expressions_columns_GSE75192.tsv\"\n",
    "val mouseRowsValidationPath = jenageValidationPath + \"/\" + \"expressions_rows_GSE75192.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "goTable = [go: string, namespace: string ... 27 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "error: error while loading KVIterator, class file '/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.0.jar(org/apache/spark/unsafe/KVIterator.class)' has location not matching its contents: contains class KVIterator\n",
       "error: error while loading UnsafeAlignedOffset, class file '/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.0.jar(org/apache/spark/unsafe/UnsafeAlignedOffset.class)' has location not matching its contents: contains class UnsafeAlignedOffset\n",
       "error: error while loading Platform, class file '/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.0.jar(org/apache/spark/unsafe/Platform.class)' has location not matching its contents: contains class Platform\n",
       "error: error while loading UTF8String, class file '/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.0.jar(org/apache/spark/unsafe/types/UTF8String.class)' has location not matching its contents: contains class UTF8String\n",
       "error: error while loading CalendarInterval, class file '/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.0.jar(org/apache/spark/unsafe/types/CalendarInterval.class)' has location not matching its contents: contains class CalendarInterval\n",
       "error: error while loading ByteArray, class file '/usr/local/spark-2.3.0-bin-hadoop2.7/jars/spark-unsafe_2.11-2.3.0.jar(org/apache/spark/unsafe/types/ByteArray.class)' has location not matching its contents: contains class ByteArray\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[go: string, namespace: string ... 27 more fields]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val goTable = readTSV(az(comparison), true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gtexPath = /GTEx\n",
       "transcriptsPath = wasbs://storage@pipelines1.blob.core.windows.net//GTEx/GTEx_Analysis_2016-01-15_v7_RSEMv1.2.22_transcript_tpm.txt\n",
       "gctPath = wasbs://storage@pipelines1.blob.core.windows.net//GTEx/GTEx_Analysis_2016-01-15_v7_RNASeQCv1.1.8_gene_tpm.gct\n",
       "transcripts2EntrezPath = wasbs://storage@pipelines1.blob.core.windows.net//GTEx/gencode.v28lift37.metadata.EntrezGene\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "wasbs://storage@pipelines1.blob.core.windows.net//GTEx/gencode.v28lift37.metadata.EntrezGene"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val gtexPath = \"/GTEx\"\n",
    "//val genesPath = az(gtexPath + \"/gtex_genes.tsv\")\n",
    "val transcriptsPath = az(gtexPath + \"/GTEx_Analysis_2016-01-15_v7_RSEMv1.2.22_transcript_tpm.txt\")\n",
    "val gctPath = az(gtexPath + \"/GTEx_Analysis_2016-01-15_v7_RNASeQCv1.1.8_gene_tpm.gct\")\n",
    "val transcripts2EntrezPath = az(gtexPath + \"/gencode.v28lift37.metadata.EntrezGene\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "simplifyFunction: org.apache.spark.sql.expressions.UserDefinedFunction\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def simplifyFunction = udf[String, String]{case tran =>\n",
    "  val i = tran.indexOf('|')\n",
    "  val tr = if(i> 0) tran.substring(0, tran.indexOf('|')) else tran\n",
    "  tr.substring(0, Math.min(tr.indexOf('.'),tr.length))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.io.IOException\n",
       "Message: No FileSystem for scheme: wasbs\n",
       "StackTrace:   at org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2660)\n",
       "  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2667)\n",
       "  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:94)\n",
       "  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2703)\n",
       "  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2685)\n",
       "  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:373)\n",
       "  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:295)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:705)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n",
       "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n",
       "  at scala.collection.immutable.List.foreach(List.scala:381)\n",
       "  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n",
       "  at scala.collection.immutable.List.flatMap(List.scala:344)\n",
       "  at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n",
       "  at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n",
       "  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n",
       "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:594)\n",
       "  at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:473)\n",
       "  at readTSV(<console>:85)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conversions = readTSV(transcripts2EntrezPath, false).toDF(\"transcript_id\", \"entrez\")//.persist(StorageLevel.MEMORY_AND_DISK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147543"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversions.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19165"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversions.select(\"entrez\").distinct.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transcripts = [transcript_id: string, gene_id: string ... 11689 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[transcript_id: string, gene_id: string ... 11689 more fields]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val transcripts = readTSV(transcriptsPath, true).join(conversions, Seq(\"transcript_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cols = transcripts.columns.tail.tail.filter(_!=\"entrez\")\n",
    "val genes = transcripts.groupBy(\"entrez\").sum(cols:_*).persist(StorageLevel.MEMORY_AND_DISK) \n",
    "val renamings = genes.columns.map{ case value => if(value.startsWith(\"sum(\")) value.replace(\"sum(\", \"\").replace(\")\",\"\") else value}\n",
    "val genesEntrez = genes.toDF(renamings:_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write(genesEntrez, az(gtexPath + \"/genes_entrez.tsv\"), true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genesEntrez = [entrez: int, GTEX-1117F-0226-SM-5GZZ7: double ... 11687 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[entrez: int, GTEX-1117F-0226-SM-5GZZ7: double ... 11687 more fields]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val genesEntrez = readTSV(az(gtexPath + \"/genes_entrez.tab\"), true).persist(StorageLevel.MEMORY_AND_DISK) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17459"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genesEntrez.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11689"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "genesEntrez.columns.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val stats = genesEntrez.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write(stats, az(gtexPath + \"/genes_entrez_stats.tsv\"), true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Name: java.lang.OutOfMemoryError\n",
       "Message: Java heap space\n",
       "StackTrace:   at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:141)\n",
       "  at scala.reflect.ManifestFactory$$anon$12.newArray(Manifest.scala:139)\n",
       "  at breeze.linalg.DenseVector$mcD$sp.<init>(DenseVector.scala:60)\n",
       "  at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeGramianMatrix(RowMatrix.scala:122)\n",
       "  at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeCovariance(RowMatrix.scala:344)\n",
       "  at org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponentsAndExplainedVariance(RowMatrix.scala:387)\n",
       "  at org.apache.spark.mllib.feature.PCA.fit(PCA.scala:53)\n",
       "  at org.apache.spark.ml.feature.PCA.fit(PCA.scala:99)\n",
       "  at fitPCA(<console>:144)\n",
       "  at doPCA(<console>:148)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pca100 = doPCA(genesEntrez, genesEntrez.columns.tail, 100)(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "println(\"Hello!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation matrix:\n",
      " 1.0                 1.0                 0.9999999999999998  \n",
      "1.0                 1.0                 0.9999999999999998  \n",
      "0.9999999999999998  0.9999999999999998  1.0                 \n",
      "Spearman correlation matrix:\n",
      " 1.0                 1.0000000000000002  1.0000000000000002  \n",
      "1.0000000000000002  1.0                 1.0000000000000002  \n",
      "1.0000000000000002  1.0000000000000002  1.0                 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "data = List([4.0,5.0,3.0], [6.0,7.0,8.0], [6.0,7.0,8.0])\n",
       "df = [features: vector]\n",
       "coeff1 = \n",
       "coeff2 = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "1.0                 1.0000000000000002  1.0000000000000002  \n",
       "1.0000000000000002  1.0                 1.0000000000000002  \n",
       "1.0000000000000002  1.0000000000000002  1.0                 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.linalg.{Matrix, Vectors}\n",
    "import org.apache.spark.ml.stat.Correlation\n",
    "import org.apache.spark.sql.Row\n",
    "\n",
    "val data = Seq(\n",
    "  Vectors.dense(4.0, 5.0, 3.0),\n",
    "  Vectors.dense(6.0, 7.0,  8.0),\n",
    "  Vectors.dense(6.0, 7.0,  8.0)  \n",
    ")\n",
    "\n",
    "val df = data.map(Tuple1.apply).toDF(\"features\")\n",
    "val Row(coeff1: Matrix) = Correlation.corr(df, \"features\").head\n",
    "println(s\"Pearson correlation matrix:\\n $coeff1\")\n",
    "\n",
    "val Row(coeff2: Matrix) = Correlation.corr(df, \"features\", \"spearman\").head\n",
    "println(s\"Spearman correlation matrix:\\n $coeff2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
