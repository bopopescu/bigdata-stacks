{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enrichment\n",
    "=========="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available line magics:\n",
      "%activity  %cd  %connect_info  %conversation  %dot  %download  %edit  %get  %help  %html  %include  %install  %install_magic  %javascript  %jigsaw  %kernel  %kx  %latex  %load  %ls  %lsmagic  %macro  %magic  %matplotlib  %parallel  %plot  %pmap  %px  %python  %reload_magics  %restart  %run  %scala  %scheme  %set  %shell\n",
      "\n",
      "Available cell magics:\n",
      "%%activity  %%brain  %%conversation  %%debug  %%dot  %%file  %%help  %%html  %%init_spark  %%javascript  %%kx  %%latex  %%macro  %%pipe  %%processing  %%px  %%python  %%scala  %%scheme  %%shell  %%show  %%time  %%tutor\n"
     ]
    }
   ],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://39ca38b8bb7d:4040\n",
       "SparkContext available as 'sc' (version = 2.3.1, master = local[*], app id = local-1531318983031)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "<console>",
     "evalue": "2: error: ';' expected but double literal found.",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: ';' expected but double literal found.",
      "%AddDeps org.bdgenomics.adam adam-core-spark2_2.11 0.24.0 --transitive",
      "                                               ^",
      "<console>:3: error: ';' expected but double literal found.",
      "%AddDeps comp.bio.aging adam-playground_2.11 0.0.13 --repository https://dl.bintray.com/comp-bio-aging/main/",
      "                                         ^",
      ""
     ]
    }
   ],
   "source": [
    "%AddDeps org.bdgenomics.adam adam-core-spark2_2.11 0.24.0 --transitive\n",
    "%AddDeps comp.bio.aging adam-playground_2.11 0.0.13 --repository https://dl.bintray.com/comp-bio-aging/main/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking org.apache.hadoop:hadoop-azure:2.7.6 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree-tmp-dir284361146908668336/toree_add_deps/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /tmp/toree-tmp-dir284361146908668336/toree_add_deps/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/2.7.6/hadoop-azure-2.7.6.jar\n",
      "-> New file at /tmp/toree-tmp-dir284361146908668336/toree_add_deps/https/repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/2.7.6/hadoop-azure-2.7.6.pom\n",
      "Marking com.microsoft.azure:azure-storage:2.0.0 for download\n",
      "Preparing to fetch from:\n",
      "-> file:/tmp/toree-tmp-dir284361146908668336/toree_add_deps/\n",
      "-> https://repo1.maven.org/maven2\n",
      "-> New file at /tmp/toree-tmp-dir284361146908668336/toree_add_deps/https/repo1.maven.org/maven2/com/microsoft/azure/azure-storage/2.0.0/azure-storage-2.0.0.jar\n",
      "-> New file at /tmp/toree-tmp-dir284361146908668336/toree_add_deps/https/repo1.maven.org/maven2/com/microsoft/azure/azure-storage/2.0.0/azure-storage-2.0.0.pom\n"
     ]
    }
   ],
   "source": [
    "%AddDeps org.apache.hadoop hadoop-azure 2.7.6\n",
    "%AddDeps com.microsoft.azure azure-storage 2.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General functions\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark._\n",
    "import org.apache.spark.sql.{DataFrame, Encoders, SparkSession}\n",
    "import org.apache.spark.sql.types.StructType\n",
    "import scala.reflect.runtime.universe._\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.rdd._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkHadoopConf: (sc: org.apache.spark.SparkContext, acountName: String, accountKey: String)org.apache.spark.SparkContext\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def sparkHadoopConf(sc: SparkContext, acountName: String, accountKey: String) = {\n",
    "  sc.hadoopConfiguration.set(\"fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\")\n",
    "  sc.hadoopConfiguration.set(\"fs.azure.account.key.\" + acountName + \".blob.core.windows.net\", accountKey)\n",
    "  sc\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azurize: (container: String, accountName: String, blobFile: String)String\n",
       "writeText2Azure: [T](rdd: org.apache.spark.rdd.RDD[T], container: String, accountName: String, blobFile: String)String\n",
       "writeTsv2Azure: (df: org.apache.spark.sql.DataFrame, container: String, accountName: String, blobFile: String)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def azurize(container: String, accountName: String, blobFile: String): String = \"wasbs://\"+container+\"@\"+accountName+\".blob.core.windows.net/\"+blobFile \n",
    "\n",
    "def writeText2Azure[T]( rdd: RDD[T], container: String, accountName: String, blobFile: String ): String =\n",
    "{\n",
    "  val url = azurize(container, accountName, blobFile)\n",
    "  rdd.saveAsTextFile(url)\n",
    "  url\n",
    "}\n",
    "\n",
    "def writeTsv2Azure( df: DataFrame, container: String, accountName: String, blobFile: String ): String =\n",
    "{\n",
    "  val url = azurize(container, accountName, blobFile)\n",
    "  df.write.option(\"sep\",\"\\t\").option(\"header\",\"true\").csv(url)\n",
    "  url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "account = pipelines1\n",
       "key = \n",
       "connString = DefaultEndpointsProtocol=https;AccountName=pipelines1;AccountKey=;EndpointSuffix=core.windows.net\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "az: (path: String)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "DefaultEndpointsProtocol=https;AccountName=pipelines1;AccountKey=;EndpointSuffix=core.windows.net"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val account = \"pipelines1\"\n",
    "val key = \"\"\n",
    "val connString = s\"DefaultEndpointsProtocol=https;AccountName=pipelines1;AccountKey=${key};EndpointSuffix=core.windows.net\"\n",
    "def az(path: String): String = azurize(\"storage\", account, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "write: (df: org.apache.spark.sql.DataFrame, url: String, header: Boolean)String\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def write(df: DataFrame, url: String, header: Boolean = true) = {\n",
    "  df.coalesce(1).write.option(\"sep\",\"\\t\").option(\"header\", header).csv(url)\n",
    "  url\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "readTSV: (path: String, header: Boolean, sep: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def readTSV(path: String, header: Boolean = false, sep: String = \"\\t\"): DataFrame = spark.read\n",
    "    .option(\"sep\", sep)\n",
    "    .option(\"comment\", \"#\")\n",
    "    .option(\"inferSchema\", true)\n",
    "    .option(\"header\", header)\n",
    "    .option(\"ignoreLeadingWhiteSpace\", true)\n",
    "    .option(\"ignoreTrailingWhiteSpace\", true)\n",
    "    .option(\"ignoreTrailingWhiteSpace\", true)\n",
    "    .option(\"maxColumns\", 150000)\n",
    "    .csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toVectors: (dataFrame: org.apache.spark.sql.DataFrame, columns: Seq[String], output: String)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def toVectors(dataFrame: DataFrame, columns: Seq[String], output: String) = {  \n",
    "  import org.apache.spark.ml.feature.VectorAssembler\n",
    "  import org.apache.spark.ml.linalg.Vectors\n",
    "  val assembler = new VectorAssembler()\n",
    "    .setInputCols(columns.toArray)\n",
    "    .setOutputCol(output)\n",
    "  assembler.transform(dataFrame.na.fill(0.0, columns).na.fill(0.0)).select(output)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sparkContext = org.apache.spark.SparkContext@1d7899e1\n",
       "spark = org.apache.spark.sql.SparkSession@24f4628b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/html": [
       "<ul>\n",
       "<li><a href=\"Some(http://39ca38b8bb7d:4040)\" target=\"new_tab\">Spark UI: app-20180711141935-0000</a></li>\n",
       "</ul>"
      ],
      "text/plain": [
       "Spark app-20180711141935-0000: Some(http://39ca38b8bb7d:4040)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sparkContext = sc\n",
    "sparkHadoopConf(sparkContext, account, key)\n",
    "  \n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"mapping_models\")\n",
    "  .getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rename: (dataFrame: org.apache.spark.sql.DataFrame, renamings: Map[String,String])org.apache.spark.sql.DataFrame\n",
       "renameFunction: (renamings: Map[String,String])org.apache.spark.sql.expressions.UserDefinedFunction\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "def rename(dataFrame: DataFrame, renamings: Map[String, String]) =   {\n",
    "      val newColumns = dataFrame.columns.map(c=> renamings.getOrElse(c, c))\n",
    "      dataFrame.toDF(newColumns:_*)\n",
    "    }\n",
    "def renameFunction(renamings: Map[String, String]) = udf[String, String](c=> renamings.getOrElse(c, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "expressionsPath = expressions\n",
       "byGoPath = expressions/go\n",
       "comparison = expressions/go/gray_whale_with_bowhead_with_minke_with_NMR_with_human_with_mouse_with_cow_full_outer_counts_extended.tsv\n",
       "grouped = expressions/go/grouped\n",
       "ranked = expressions/go/grouped/ranked\n",
       "transcriptsPath = expressions/transcripts\n",
       "indexesPath = indexes\n",
       "reactomePath = indexes/reactome\n",
       "unirefPath = expressions/uniref90\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "expressions/uniref90"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val expressionsPath = \"expressions\"\n",
    "val byGoPath = expressionsPath + \"/go\"\n",
    "val comparison = byGoPath + \"/gray_whale_with_bowhead_with_minke_with_NMR_with_human_with_mouse_with_cow_full_outer_counts_extended.tsv\"\n",
    "val grouped = byGoPath + \"/grouped\"\n",
    "val ranked = byGoPath + \"/grouped/ranked\"\n",
    "val transcriptsPath = expressionsPath + \"/transcripts\"\n",
    "val indexesPath = \"indexes\"\n",
    "val reactomePath = indexesPath + \"/reactome\"\n",
    "val unirefPath = expressionsPath + \"/\" + \"uniref90\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LAGs comparisons\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "Name: org.apache.spark.SparkException\n",
       "Message: Job aborted due to stage failure: Task 0 in stage 4.0 failed 4 times, most recent failure: Lost task 0.3 in stage 4.0 (TID 19, 10.0.1.11, executor 142): ExecutorLostFailure (executor 142 exited caused by one of the running tasks) Reason: Unable to create executor due to Exception thrown in awaitResult: \n",
       "Driver stacktrace:\n",
       "StackTrace:   at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1590)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1589)\n",
       "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
       "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1589)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
       "  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n",
       "  at scala.Option.foreach(Option.scala:257)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1823)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)\n",
       "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)\n",
       "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n",
       "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n",
       "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n",
       "  at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
       "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
       "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n",
       "  at org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n",
       "  at org.bdgenomics.adam.converters.FastaConverter$.getDescriptionLines(FastaConverter.scala:182)\n",
       "  at org.bdgenomics.adam.converters.FastaConverter$.apply(FastaConverter.scala:115)\n",
       "  at org.bdgenomics.adam.rdd.ADAMContext$$anonfun$loadFasta$1.apply(ADAMContext.scala:2485)\n",
       "  at org.bdgenomics.adam.rdd.ADAMContext$$anonfun$loadFasta$1.apply(ADAMContext.scala:2472)\n",
       "  at scala.Option.fold(Option.scala:158)\n",
       "  at org.apache.spark.rdd.Timer.time(Timer.scala:48)\n",
       "  at org.bdgenomics.adam.rdd.ADAMContext.loadFasta(ADAMContext.scala:2472)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.bdgenomics.adam.rdd.ADAMContext._\n",
    "val genage = sc.loadFasta(az(\"indexes/genage/mouse_fly_worm_genage.fasta\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://github.com/calysto/metakernel/blob/master/metakernel/magics/README.md"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
